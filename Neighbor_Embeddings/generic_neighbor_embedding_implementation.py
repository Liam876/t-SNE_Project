# -*- coding: utf-8 -*-
"""Generic_Neighbor_Embedding_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Sy5UpY_6GWqfrsq55O1poLC0JU81Uyf

Implementation of neighbor embedding that allows custom probabilities (both high and low), different optimization methods, different kernels (as for now only for high - dimension) and as a result also weighted MLE.
"""

import numpy as np


class NE:
    """
    Neighbor Embedding (NE) class for dimensionality reduction.

    Parameters:
        - num_dimensions (int): The number of dimensions in the low-dimensional space.
        - perplexity (float): for gaussian kernel in high dimensional similarity matrix
        - k (int): number of neighbors to consider in k- symmetrical nearest neighbors graph
        - learning_rate (float): for optimization algorithms
        - n_iter (int): number of iterations to perform when optimizing
        - min_grad_norm (float): minimum gradient norm that determines when to stop optimization
        - init (string/ nd_array of shape (n_samples,n_components)): determines the initialization method.
          Choose "random" for a random initialization or initialize with custom points.
        - high_sim (string): method to compute the high dimensional similarity. high_sim= "knn" runs
          symmetrical nearest neighbors graph, "gaussian" uses gaussian kernel with specified perplexity.
        - low_sim (string): method to compute the low dimensional similarities. low_sim = "cauchy" uses
          Cauchy kernel 1/(||x_i - x_j||^2 + 1) and "gaussian" that uses variance = 1/sqrt(2) as in original paper.
        - seed (int): Seed for reproducibility.

    Attributes:
        - num_dimensions (int): The number of dimensions in the low-dimensional space.
        - embedding (numpy.ndarray): The low-dimensional final embedding.
        - kl_div (float): the value of the kl-divergence of result.
        - learning_rate (float): learning rate.
        - n_iter (int): number of iterations run.
        - X (numpy.ndarray): The input data matrix.
        - P (numpy.ndarray): The high-dimensional similarity matrix.
        - Q (numpy.ndarray): The low-dimensional similarity matrix.

    Methods:
        - kl_divergence_loss(P, Q): Compute the KL divergence loss between P and Q.
        - compute_pairwise_distances(X): Compute pairwise Euclidean distances between points in X.
        - calculate_gaussian_kernel(X, perplexity): Calculate the Gaussian kernel for each pair of points.
        - compute_similarity_matrix(mode, X, perplexity=30.0, k=7): Compute the normalized similarity matrix
          using a Gaussian/Cauchy/sknn graph kernel.
        - set_custom_P(custom_P): Set a custom high-dimensional similarity matrix.
        - set_custom_Q(custom_Q): Set a custom low-dimensional similarity matrix.
        - gradient_descent(): Optimize using gradient descent.
        - gd_momentum(momentum=0.8): Optimize using gradient descent with momentum.
        - adam(beta1=0.9, beta2=0.999, epsilon=1e-4): Optimize using the Adam optimizer.
        - fit(X): Fit the high-dimensional data.
        - fit_transform(X, opt_method="gd"): Fit the high-dimensional data and create the embedding.

    """

    def __init__(self, num_dimensions, perplexity=30.0, k=7, learning_rate=200, n_iter=1000, min_grad_norm=1e-7,
                 init="random",high_sim = "gaussian", low_sim = "cauchy", seed=42):
        """
        Initialize the NE class.
        """
        np.random.seed(seed)
        self.num_dimensions = num_dimensions
        self.perplexity = perplexity
        self.k = k
        self.learning_rate = learning_rate
        self.n_iter = n_iter
        self.min_grad_norm = min_grad_norm
        self.init = init
        self.high_sim = high_sim
        self.low_sim = low_sim
        self.kl_div = None
        self.P = None
        self.Q = None

    @staticmethod
    def kl_divergence_loss(p, q):
        """
        Compute the KL divergence loss between P and Q.

        Parameters:
            - P (numpy.ndarray): High-dimensional similarity matrix.
            - Q (numpy.ndarray): Low-dimensional similarity matrix.

        Returns:
            - float: KL divergence loss.
        """
        return np.sum(p * np.log(p / q))

    @staticmethod
    def compute_pairwise_distances(X):
        """
        Compute pairwise Euclidean distances between points in X.

        Parameters:
            - X (numpy.ndarray): Data matrix.

        Returns:
            - numpy.ndarray: Pairwise distances.
        """
        distances = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))
        return distances

    @staticmethod
    def calculate_gaussian_kernel(X, perplexity):
        # Calculate pairwise Euclidean distances
        pairwise_distances = np.linalg.norm(X[:, np.newaxis, :] - X[np.newaxis, :, :], axis=2)

        # Initialize variables
        P_matrix = np.zeros_like(pairwise_distances)
        beta_matrix = np.ones_like(pairwise_distances)

        # Set tolerance for binary search
        tol = 1e-5

        # Loop through each point
        for i in range(X.shape[0]):
            beta_min = -np.inf
            beta_max = np.inf
            beta = 1.0

            # Binary search to find the optimal beta (variance) for the current point
            for _ in range(50):
                # Calculate Gaussian kernel and conditional probabilities
                exp_distances = np.exp(-beta * pairwise_distances[i, :])
                sum_exp_distances = np.sum(exp_distances)
                P_matrix[i, :] = exp_distances / sum_exp_distances

                # Calculate Shannon entropy of the distribution
                H_Pi = -np.sum(P_matrix[i, :] * np.log2(P_matrix[i, :] + 1e-15))

                # Update beta based on the perplexity constraint
                H_diff = H_Pi - np.log2(perplexity)
                if np.abs(H_diff) < tol:
                    break
                elif H_diff > 0:
                    beta_min = beta
                    if beta_max == np.inf:
                        beta *= 2
                    else:
                        beta = (beta + beta_max) / 2
                else:
                    beta_max = beta
                    if beta_min == -np.inf:
                        beta /= 2
                    else:
                        beta = (beta + beta_min) / 2

            # Store the optimal beta for the current point
            beta_matrix[i, :] = beta

        return P_matrix, beta_matrix

    @staticmethod
    def compute_similarity_matrix(mode, X, perplexity=30.0, k=7):
        """
        Compute the normalized similarity matrix using a Gaussian/Cauchy/sknng kernel.
        P[i,j] = probability of pair (i,j)

        Parameters:
            - X (numpy.ndarray): Data matrix.
            - sigma (float): Width of the Gaussian kernel.

        Returns:
            - numpy.ndarray: Similarity matrix.
        """
        distances = NE.compute_pairwise_distances(X)
        n = distances.shape[0]
        P = np.zeros((n, n))

        if mode == "gaussian":
            P,_ = NE.calculate_gaussian_kernel(X,perplexity)
            P += P.T
            P /= (2 * n)
        elif mode == "knn":
            for i in range(n):
                sorted_indices = np.argsort(distances[i, :])
                P[i, sorted_indices[1:k + 1]] = 1
            P = 2 * P / np.sum(P)
        elif mode == "cauchy":
            P = distances + 1
            P = np.reciprocal(P)
            P = 2*P/ np.sum(P)
        return P

    def fit(self,X):
        """
        Fit the high dimensional data.

        parameters:
            - X (numpy.ndarray): data to be visualized
        """

        self.X = X
        self.n_points = X.shape[0]
        self.P = NE.compute_similarity_matrix(self.high_sim,self.X,self.perplexity,self.k)
        if type(self.init) ==str and self.init == "random":
            self.embedding = np.random.normal(loc = [0,0],scale = np.sqrt(1e-4),size = (self.n_points,self.num_dimensions))
        else:
            self.embedding = self.init

        self.Q = NE.compute_similarity_matrix(self.low_sim,self.embedding,self.perplexity,self.k)

    def fit_transform(self,X, opt_method = "gd"):
        """
        Fit the high dimensional data and create the embedding

        parameters:
            - X (numpy.ndarray): data to be visualized
            - opt_method (string): the optimization method to run neighbor embedding. Options are:
              "gd", "gd_momentum", "adam"
        """
        self.fit(X)
        if opt_method == "gd":
            self.gradient_descent()
        elif opt_method == "gd_momentum":
            self.gd_momentum()
        else:
            self.adam()

    def set_custom_P(self, custom_P):
        """
        Set a custom high-dimensional similarity matrix.

        Parameters:
            - custom_P (numpy.ndarray): Custom high-dimensional similarity matrix.
        """
        self.P = custom_P

    def set_custom_Q(self, custom_Q):
        """
        Set a custom low-dimensional similarity matrix.

        Parameters:
            - custom_Q (numpy.ndarray): Custom low-dimensional similarity matrix.
        """
        self.Q = custom_Q

    def gradient_descent(self):
        """
        Optimize using gradient descent.

        Parameters:
            - learning_rate (float): Learning rate for gradient descent.
            - num_iterations (int): Number of iterations.
        """
        for iteration in range(self.n_iter):
            loss = NE.kl_divergence_loss(self.P, self.Q)
            self.kl_div = loss
            print(f"Iteration {iteration + 1}, Loss: {loss}")

            grad = np.zeros_like(self.embedding)
            Q = self.Q
            for i in range(self.embedding.shape[0]):
                diff = self.embedding[i, :] - self.embedding
                grad[i, :] = np.sum((self.P[i, :] - Q[i, :])[:, np.newaxis] * diff, axis=0)

            self.embedding -= self.learning_rate * grad

    def gd_momentum(self, momentum= 0.8):
        """
        Optimize using gradient descent with momentum.

        Parameters:
            - learning_rate (float): Learning rate for gradient descent.
            - momentum (float): Momentum factor.
            - num_iterations (int): Number of iterations.
        """
        velocity = np.zeros_like(self.embedding)

        for iteration in range(self.n_iter):
            loss = NE.kl_divergence_loss(self.P, self.Q)
            self.kl_div = loss
            print(f"Iteration {iteration + 1}, Loss: {loss}")

            grad = np.zeros_like(self.embedding)
            for i in range(self.embedding.shape[0]):
                diff = self.embedding[i, :] - self.embedding
                grad[i, :] = np.sum((self.P[i, :] - self.Q[i, :])[:, np.newaxis] * diff, axis=0)

            velocity = momentum * velocity + self.learning_rate * grad
            self.embedding -= velocity

    def adam(self, beta1=0.9, beta2=0.999, epsilon=1e-4):
        """
        Optimize using the Adam optimizer.

        Parameters:
            - learning_rate (float): Learning rate for Adam optimizer.
            - beta1 (float): Exponential decay rate for the first moment estimates.
            - beta2 (float): Exponential decay rate for the second moment estimates.
            - epsilon (float): Small constant to prevent division by zero.
            - num_iterations (int): Number of iterations.
        """
        m = np.zeros_like(self.embedding)
        v = np.zeros_like(self.embedding)
        t = 0

        for iteration in range(self.n_iter):
            loss = NE.kl_divergence_loss(self.P, self.Q)
            self.kl_div = loss
            print(f"Iteration {iteration + 1}, Loss: {loss}")

            grad = np.zeros_like(self.embedding)
            for i in range(self.embedding.shape[0]):
                diff = self.embedding[i, :] - self.embedding
                grad[i, :] = np.sum((self.P[i, :] - self.Q[i, :])[:, np.newaxis] * diff, axis=0)

            t += 1
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * (grad ** 2)
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)

            self.embedding -= self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)