{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Tc51rgF-o_Fa",
        "oJEN07iXpF0K"
      ],
      "authorship_tag": "ABX9TyOcCmNYC4yX5g0cV8XXeCzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liam876/t-SNE_Project/blob/main/notebooks/Simple_Classifiers_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "WAJbcfhgov9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLZ3GXCvZaDo"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.ticker import NullFormatter\n",
        "from sklearn import datasets, manifold\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, MaxPooling1D, Reshape\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIDRKIhLqB4W",
        "outputId": "1caa3ab4-e56d-4a06-d6d6-50ae5d741b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data"
      ],
      "metadata": {
        "id": "CZJ1vLeoqRop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_to_X_init1 =\"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_init1\"\n",
        "path_to_X_ex1 =  \"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_ex1\"\n",
        "path_to_y1 = \"/content/drive/MyDrive/tsne_circles/Circles_csvs/y1\"\n",
        "X_init1 = pd.read_csv(path_to_X_init1, header= None).values\n",
        "X_ex1 = pd.read_csv(path_to_X_ex1, header= None).values\n",
        "y1 = pd.read_csv(path_to_y1, header= None).values\n",
        "\n",
        "path_to_X_init2 =\"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_init2\"\n",
        "path_to_X_ex2 =  \"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_ex2\"\n",
        "path_to_y2 = \"/content/drive/MyDrive/tsne_circles/Circles_csvs/y2\"\n",
        "X_init2 = pd.read_csv(path_to_X_init2, header= None).values\n",
        "X_ex2 = pd.read_csv(path_to_X_ex2, header= None).values\n",
        "y2 = pd.read_csv(path_to_y2, header= None).values\n",
        "\n",
        "path_to_X_init3 =\"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_init3\"\n",
        "path_to_X_ex3 =  \"/content/drive/MyDrive/tsne_circles/Circles_csvs/X_ex3\"\n",
        "path_to_y3 = \"/content/drive/MyDrive/tsne_circles/Circles_csvs/y3\"\n",
        "X_init3 = pd.read_csv(path_to_X_init3, header= None).values\n",
        "X_ex3 = pd.read_csv(path_to_X_ex3, header= None).values\n",
        "y3 = pd.read_csv(path_to_y3, header= None).values"
      ],
      "metadata": {
        "id": "hUYEFmOUqEeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_init = np.vstack((X_init1,X_init2,X_init3))\n",
        "X_ex = np.vstack((X_ex1,X_ex2,X_ex3))\n",
        "y = np.concatenate((y1,y2,y3))"
      ],
      "metadata": {
        "id": "KKNu5CVw_O-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ex.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKqnrlZp_ywh",
        "outputId": "c975f24f-ac75-4f75-96a1-1d85c7417845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{round(np.sum(y)*100/len(y),2)}% are 1-labeled examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Mh16ta62lf",
        "outputId": "f2791ce4-f214-4bbd-ca56-3df5d5347d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44.23% are 1-labeled examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "Qfh98wbbo28z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feedforward"
      ],
      "metadata": {
        "id": "Tc51rgF-o_Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ex, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=300, input_dim=300, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "#custom_optimizer = Adam(learning_rate=10**-3)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "LJhzlmLi68g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conv"
      ],
      "metadata": {
        "id": "oJEN07iXpF0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "    input_shape = (300,)\n",
        "\n",
        "    model = Sequential([\n",
        "    # Reshape the 1D input into a 2D input\n",
        "    Reshape((75, 4), input_shape=input_shape),\n",
        "\n",
        "    # Convolutional layer with 64 filters and a kernel size of 3\n",
        "    Conv1D(64, kernel_size=25, activation='relu'),\n",
        "    #MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Convolutional layer with 128 filters and a kernel size of 3\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    #MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Convolutional layer with 256 filters and a kernel size of 3\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Flatten the output to connect to dense layers\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense layer with 256 units, ReLU activation, and dropout for regularization\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # # Dense layer with 128 units and ReLU activation\n",
        "    # Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer with a single neuron for binary classification\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "j10uPmNF7zll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assuming X and y are defined\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ex, y, test_size=0.2, random_state=42)\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "custom_optimizer = Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "4jjsZjtY73eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribution- Based"
      ],
      "metadata": {
        "id": "dZkEXX1vI4YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing the data and creating constants\n",
        "n_samples = 100\n",
        "start = 900\n",
        "X_data = X_init[start:start+n_samples]\n",
        "y_data = y[start:start+n_samples]\n",
        "n_points = int(X_data.shape[1]/2)\n",
        "n_pairs = int((n_points**2 - n_points)/2)\n",
        "\n",
        "#Calculate the Cauchy similarity between 2 points\n",
        "def cauchy_similarity(x,y,sigma=1.0):\n",
        "  dist = np.linalg.norm(x-y)\n",
        "  sim = 1/(1+ (dist/sigma))\n",
        "  return sim\n",
        "\n",
        "#Transform the points into distribution\n",
        "def transform_sample(sample):\n",
        "  #print(type(sample))\n",
        "  #print(sample)\n",
        "  points = sample.reshape((n_points,2))\n",
        "  res = np.zeros((n_pairs))\n",
        "  part_fun = 0\n",
        "  idx = 0\n",
        "\n",
        "  for i in range(n_points):\n",
        "    for j in range(i+1,n_points):\n",
        "      sim = cauchy_similarity(points[i],points[j])\n",
        "      res[idx] = sim\n",
        "      part_fun += sim\n",
        "      idx +=1\n",
        "\n",
        "  assert res[-1] != 0 # Just to make sure the list is full\n",
        "  res = res/part_fun # Normalize the distribution\n",
        "  return res\n",
        "\n",
        "# Generator function for on-the-fly data transformation\n",
        "def data_generator(X, y, batch_size=32):\n",
        "    while True:\n",
        "        indices = np.random.choice(len(X), batch_size, replace=False)\n",
        "        batch_X = X[indices]\n",
        "        batch_y = y[indices]\n",
        "\n",
        "        # Perform on-the-fly transformation for each sample in the batch\n",
        "        transformed_batch_X = np.array([transform_sample(sample) for sample in batch_X])\n",
        "\n",
        "        yield transformed_batch_X, batch_y\n",
        "class TopKMaskingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, k, **kwargs):\n",
        "        super(TopKMaskingLayer, self).__init__(**kwargs)\n",
        "        self.k = k\n",
        "\n",
        "    def call(self, inputs):\n",
        "        top_k_values, top_k_indices = tf.nn.top_k(inputs, k=self.k)\n",
        "        #mask = tf.cast(tf.math.less(tf.range(tf.shape(inputs)[-1])[:, tf.newaxis], top_k_indices), dtype=tf.float32)\n",
        "        return top_k_values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "validation_data = np.array([transform_sample(s) for s in X_test]).reshape((X_test.shape[0],-1))\n",
        "# Model definition\n",
        "\n",
        "class MaxThresholdLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, threshold_value, **kwargs):\n",
        "        super(MaxThresholdLayer, self).__init__(**kwargs)\n",
        "        self.threshold_value = threshold_value\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.maximum(inputs, self.threshold_value)\n",
        "\n",
        "max_threshold_layer = tf.keras.layers.Lambda(lambda x: tf.maximum(x, 0.01), input_shape=(n_pairs,))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    #TopKMaskingLayer(k=1000, input_shape=(n_pairs,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu',input_shape=(n_pairs,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the generator\n",
        "batch_size = 32\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(data_generator(X_train, y_train, batch_size),\n",
        "          epochs=500,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=(validation_data, y_test),\n",
        "           callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(validation_data)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "01xIZJkKI-WC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}